{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edf62f0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOCAL = 1 indicates running this notebook locally, 0 indicates running it on Kaggle\n",
    "LOCAL = 1\n",
    "\n",
    "import os\n",
    "if LOCAL != 1:\n",
    "  GITHUB_USER = \"magnusdtd\"\n",
    "  REPO_NAME = \"ENTRep\"\n",
    "  BRANCH_NAME = \"BioCLIP\"\n",
    "\n",
    "  from kaggle_secrets import UserSecretsClient\n",
    "  user_secrets = UserSecretsClient()\n",
    "  GITHUB_TOKEN = user_secrets.get_secret(\"GITHUB_TOKEN\")\n",
    "\n",
    "  os.system(f\"git clone --single-branch --branch {BRANCH_NAME} https://{GITHUB_USER}:{GITHUB_TOKEN}@github.com/{GITHUB_USER}/{REPO_NAME}.git\")\n",
    "  os.chdir(\"/kaggle/working/\")\n",
    "\n",
    "  from ENTRep.utils.file import File\n",
    "  File.make_train_path()\n",
    "else:\n",
    "  os.chdir(\"..\")\n",
    "\n",
    "current_path = os.getcwd()\n",
    "print(\"Current path:\", current_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "459d271f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install open_clip_torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29b8b618",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3601e565",
   "metadata": {},
   "outputs": [],
   "source": [
    "from BioCLIP.make_submission import make_submission_cls_task, make_submission_t2i_task\n",
    "from BioCLIP.evaluator import ImageToTextEvaluator, TextToImageEvaluator\n",
    "from BioCLIP.data_preparation import DataPreparation\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56fc0efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import torch\n",
    "import open_clip\n",
    "import os\n",
    "\n",
    "class ImageToTextEvaluator:\n",
    "  def __init__(\n",
    "    self, \n",
    "    df:pd.DataFrame, \n",
    "    labels: dict[str], \n",
    "    model_name:str, \n",
    "    model_path:str,\n",
    "    path_column:str,\n",
    "    caption_column:str\n",
    "  ):\n",
    "    '''\n",
    "    The df contains paths to images at column 'Path'.\n",
    "    This evaluator also known as classification evaluator.\n",
    "    '''\n",
    "    self.df = df\n",
    "    self.model_name = model_name\n",
    "    self.model_path = model_path\n",
    "    self.labels = labels\n",
    "    self.path_column = path_column\n",
    "    self.caption_column = caption_column\n",
    "\n",
    "    self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "    if self.model_path:\n",
    "      self.model, _, self.preprocess_val = open_clip.create_model_and_transforms(self.model_name, pretrained=self.model_path)\n",
    "    else:\n",
    "      self.model, _, self.preprocess_val = open_clip.create_model_and_transforms(self.model_name)\n",
    "    self.model.to(self.device)\n",
    "    self.model.eval()\n",
    "    self.tokenizer = open_clip.get_tokenizer(model_name)\n",
    "  \n",
    "  def get_accuracy(self):\n",
    "    correct_predictions = 0\n",
    "    total_predictions = 0\n",
    "\n",
    "    for _, row in self.df.iterrows():\n",
    "      image_path = row[self.path_column]\n",
    "      image_tensor = self.preprocess_val(Image.open(image_path)).unsqueeze(0).to(self.device)\n",
    "      text_tokens = self.tokenizer(self.labels).to(self.device)\n",
    "\n",
    "      with torch.no_grad():\n",
    "        image_features = self.model.encode_image(image_tensor)\n",
    "        text_features = self.model.encode_text(text_tokens)\n",
    "\n",
    "      image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "      text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "      text_probs = (100.0 * image_features @ text_features.T).softmax(dim=-1)\n",
    "      predicted_label_idx = text_probs.argmax(dim=-1).item()\n",
    "      predicted_label = self.labels[predicted_label_idx]\n",
    "\n",
    "      if predicted_label == row[self.caption_column]:\n",
    "        correct_predictions += 1\n",
    "\n",
    "      total_predictions += 1\n",
    "\n",
    "    accuracy = correct_predictions / total_predictions\n",
    "    return accuracy\n",
    "\n",
    "  def get_precision(self):\n",
    "    true_positives = 0\n",
    "    false_positives = 0\n",
    "\n",
    "    for _, row in self.df.iterrows():\n",
    "      image_path = row[self.path_column]\n",
    "      image_tensor = self.preprocess_val(Image.open(image_path)).unsqueeze(0).to(self.device)\n",
    "      text_tokens = self.tokenizer(self.labels).to(self.device)\n",
    "\n",
    "      with torch.no_grad():\n",
    "        image_features = self.model.encode_image(image_tensor)\n",
    "        text_features = self.model.encode_text(text_tokens)\n",
    "\n",
    "      image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "      text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "      text_probs = (100.0 * image_features @ text_features.T).softmax(dim=-1)\n",
    "      predicted_label_idx = text_probs.argmax(dim=-1).item()\n",
    "      predicted_label = self.labels[predicted_label_idx]\n",
    "\n",
    "      if predicted_label == row[self.caption_column]:\n",
    "        true_positives += 1\n",
    "      elif predicted_label != row[self.caption_column]:\n",
    "        false_positives += 1\n",
    "\n",
    "    precision = true_positives / (true_positives + false_positives)\n",
    "    return precision\n",
    "\n",
    "  def get_recall(self):\n",
    "    true_positives = 0\n",
    "    false_negatives = 0\n",
    "\n",
    "    for _, row in self.df.iterrows():\n",
    "      image_path = row[self.path_column]\n",
    "      image_tensor = self.preprocess_val(Image.open(image_path)).unsqueeze(0).to(self.device)\n",
    "      text_tokens = self.tokenizer(self.labels).to(self.device)\n",
    "\n",
    "      with torch.no_grad():\n",
    "        image_features = self.model.encode_image(image_tensor)\n",
    "        text_features = self.model.encode_text(text_tokens)\n",
    "\n",
    "      image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "      text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "      text_probs = (100.0 * image_features @ text_features.T).softmax(dim=-1)\n",
    "      predicted_label_idx = text_probs.argmax(dim=-1).item()\n",
    "      predicted_label = self.labels[predicted_label_idx]\n",
    "\n",
    "      if predicted_label == row[self.caption_column]:\n",
    "        true_positives += 1\n",
    "      elif predicted_label != row[self.caption_column]:\n",
    "        false_negatives += 1\n",
    "\n",
    "    recall = true_positives / (true_positives + false_negatives)\n",
    "    return recall\n",
    "\n",
    "  def get_f1_score(self):\n",
    "    precision = self.get_precision()\n",
    "    recall = self.get_recall()\n",
    "    f1_score = 2 * (precision * recall) / (precision + recall)\n",
    "    return precision, recall, f1_score\n",
    "\n",
    "\n",
    "class TextToImageEvaluator:\n",
    "  def __init__(\n",
    "      self, \n",
    "      df: pd.DataFrame, \n",
    "      queries: dict[str], \n",
    "      model_name: str, \n",
    "      model_path: str,\n",
    "      path_column: str,\n",
    "      caption_column: str\n",
    "    ):\n",
    "    '''\n",
    "    The df contains paths to images at column 'Path'.\n",
    "    '''\n",
    "    self.df = df\n",
    "    self.model_name = model_name\n",
    "    self.model_path = model_path\n",
    "    self.queries = queries\n",
    "    self.path_column = path_column\n",
    "    self.caption_column = caption_column\n",
    "    self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "    if self.model_path:\n",
    "      self.model, _, self.preprocess_val = open_clip.create_model_and_transforms(self.model_name, pretrained=self.model_path)\n",
    "    else:\n",
    "      self.model, _, self.preprocess_val = open_clip.create_model_and_transforms(self.model_name)\n",
    "    self.model.to(self.device)\n",
    "    self.model.eval()\n",
    "    self.tokenizer = open_clip.get_tokenizer(self.model_name)\n",
    "    self.query_tokens = self.tokenizer(queries).to(self.device)\n",
    "\n",
    "    self.image_features_dict = {}\n",
    "    self._feature_extract()\n",
    "\n",
    "  def _feature_extract(self):\n",
    "    for _, row in self.df.iterrows():\n",
    "      image_path = row[self.path_column]\n",
    "      image_name = os.path.basename(image_path)\n",
    "      image_tensor = self.preprocess_val(Image.open(image_path)).unsqueeze(0).to(self.device)\n",
    "\n",
    "      with torch.no_grad():\n",
    "        image_features = self.model.encode_image(image_tensor)\n",
    "\n",
    "      image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "      self.image_features_dict[image_name] = image_features\n",
    "\n",
    "  def get_recall_at_k(self, k: int):\n",
    "    num_queries_with_correct_image_in_top_k = 0\n",
    "    total_num_queries = len(self.queries)\n",
    "\n",
    "    # Debug\n",
    "    top_k_dict = {}\n",
    "\n",
    "    with torch.no_grad():\n",
    "      text_features = self.model.encode_text(self.query_tokens)\n",
    "      text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "      text_features = text_features.float()\n",
    "      for i, query in enumerate(self.queries):\n",
    "        similarities = {}\n",
    "        for image_name, image_features in self.image_features_dict.items():\n",
    "          similarity = (100.0 * text_features[i] @ image_features.T).item()\n",
    "          similarities[image_name] = similarity\n",
    "\n",
    "        # Sort images by similarity and get top-k\n",
    "        sorted_images = sorted(similarities.items(), key=lambda x: x[1], reverse=True)\n",
    "        top_k_images = [image[0] for image in sorted_images[:k]]\n",
    "\n",
    "        # Check if the correct image is in the top-k images\n",
    "        if query in top_k_images:\n",
    "          num_queries_with_correct_image_in_top_k += 1\n",
    "\n",
    "        top_k_dict[query] = top_k_images\n",
    "\n",
    "    for query, top_k_images in top_k_dict.items():\n",
    "      print(f\"{query}, {top_k_images}\")\n",
    "\n",
    "    recall_at_k = num_queries_with_correct_image_in_top_k / total_num_queries\n",
    "    return recall_at_k\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6b3a250",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_preparation = DataPreparation()\n",
    "image_to_text_df = data_preparation.preprocess_data()\n",
    "image_to_text_df = data_preparation.detect_and_translate(image_to_text_df)\n",
    "data_preparation.validate_dataframe(image_to_text_df)\n",
    "image_to_text_df['Path'] = image_to_text_df['Path'].apply(lambda x: os.path.join(\"/kaggle/working/\", x))\n",
    "image_to_text_df.head()\n",
    "queries = image_to_text_df['DescriptionEN'].to_list()\n",
    "\n",
    "text_to_image_evaluator = TextToImageEvaluator(\n",
    "    df=image_to_text_df,\n",
    "    queries=queries,\n",
    "    model_name='hf-hub:magnusdtd/bio-clip-cls-ft',\n",
    "    model_path='',\n",
    "    path_column='Path',\n",
    "    caption_column='DescriptionEN'\n",
    ")\n",
    "\n",
    "# Evaluate recall at k\n",
    "recall_at_k = text_to_image_evaluator.get_recall_at_k(k=10)\n",
    "print(f\"Recall at k: {recall_at_k}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c15621bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_to_text_df = pd.read_json('Dataset/train/cls.json', orient='index')\n",
    "image_to_text_df = image_to_text_df.reset_index()\n",
    "image_to_text_df.columns = ['Path', 'Ground Truth Label']\n",
    "image_to_text_df['Path'] = image_to_text_df['Path'].apply(lambda x: os.path.join(\"/kaggle/working/Dataset/train/imgs\", x))\n",
    "image_to_text_df\n",
    "\n",
    "labels = [\n",
    "    \"nose-right\", \n",
    "    \"nose-left\" , \n",
    "    \"ear-right\" , \n",
    "    \"ear-left\"  , \n",
    "    \"vc-open\"   , \n",
    "    \"vc-closed\" , \n",
    "    \"throat\"    , \n",
    "]\n",
    "\n",
    "image_to_text_evaluator = ImageToTextEvaluator(\n",
    "    df=image_to_text_df,\n",
    "    labels=labels,\n",
    "    model_name='hf-hub:magnusdtd/bio-clip-cls-ft',\n",
    "    model_path='',\n",
    "    path_column='Path',\n",
    "    caption_column='Ground Truth Label'\n",
    ")\n",
    "\n",
    "accuracy = image_to_text_evaluator.get_accuracy()\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "\n",
    "precision, recall, f1_score = image_to_text_evaluator.get_f1_score()\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"F1 Score: {f1_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "214adf1e",
   "metadata": {},
   "source": [
    "# Make submissions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba4814c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "import open_clip\n",
    "import os\n",
    "import json\n",
    "import zipfile\n",
    "import pandas as pd\n",
    "import datetime\n",
    "\n",
    "def make_submission_cls_task(\n",
    "    model_name: str, \n",
    "    model_path:str, \n",
    "    test_file_path: str, \n",
    "    labels_map:dict[str, int], \n",
    "    output_folder_path: str = './results'\n",
    "  ):\n",
    "  device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "  test_df = pd.read_csv(test_file_path, header=None, names=['Path'])\n",
    "\n",
    "  # Load BioCLIP model and tokenizer\n",
    "  if model_path:\n",
    "    model, _, preprocess_val = open_clip.create_model_and_transforms(model_name, pretrained=model_path)\n",
    "  else:\n",
    "    model, _, preprocess_val = open_clip.create_model_and_transforms(model_name)\n",
    "  model.to(device)\n",
    "  model.eval()\n",
    "  tokenizer = open_clip.get_tokenizer(model_name)\n",
    "\n",
    "  label_names = list(labels_map.keys())\n",
    "  predictions = {}\n",
    "  for img_name in test_df['Path']:\n",
    "    image_path = os.path.join('Dataset/test/imgs', img_name)\n",
    "    image_tensor = preprocess_val(Image.open(image_path)).unsqueeze(0).to(device)\n",
    "    text = tokenizer(label_names).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "      image_features = model.encode_image(image_tensor)\n",
    "      text_features = model.encode_text(text)\n",
    "\n",
    "    image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "    text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "    text_probs = (100.0 * image_features @ text_features.T).softmax(dim=-1)\n",
    "    best_match_idx = text_probs.argmax(dim=-1)\n",
    "    predictions[img_name] = labels_map[label_names[best_match_idx.item()]]\n",
    "\n",
    "  # Generate unique JSON filename with model_name as prefix\n",
    "  daytime = datetime.datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "  json_file_name = f'BioCLIP_cls_{daytime}.json'\n",
    "  json_file_path = os.path.join(output_folder_path, json_file_name)\n",
    "\n",
    "  # Save predictions to JSON file\n",
    "  with open(json_file_path, 'w') as json_file:\n",
    "    json.dump(predictions, json_file)\n",
    "\n",
    "  # Create ZIP archive with the same name as the JSON file\n",
    "  zip_file_path = os.path.join(output_folder_path, f'BioCLIP_cls_{daytime}.zip')\n",
    "  with zipfile.ZipFile(zip_file_path, 'w') as zip_file:\n",
    "    zip_file.write(json_file_path, arcname=json_file_name)\n",
    "\n",
    "  print(f\"Submission file created at: {zip_file_path}\")\n",
    "\n",
    "def make_submission_t2i_task(\n",
    "    model_name: str, \n",
    "    model_path: str, \n",
    "    test_file_path: list, \n",
    "    image_folder_path: str, \n",
    "    output_folder_path: str = './results'\n",
    "  ):\n",
    "  test_df = pd.read_csv(test_file_path, header=None, names=['Query'])\n",
    "\n",
    "  device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "  if model_path:\n",
    "    model, _, preprocess_val = open_clip.create_model_and_transforms(model_name, pretrained=model_path)\n",
    "  else:\n",
    "    model, _, preprocess_val = open_clip.create_model_and_transforms(model_name)\n",
    "  model.to(device)\n",
    "  model.eval()\n",
    "  tokenizer = open_clip.get_tokenizer(model_name)\n",
    "\n",
    "  # Preprocess text queries\n",
    "  text_tokens = tokenizer(test_df['Query'].to_list()).to(device)\n",
    "\n",
    "  # Extract image features\n",
    "  image_features_dict = {}\n",
    "  for image_name in os.listdir(image_folder_path):\n",
    "    image_path = os.path.join(image_folder_path, image_name)\n",
    "    image_tensor = preprocess_val(Image.open(image_path)).unsqueeze(0).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "      image_features = model.encode_image(image_tensor)\n",
    "\n",
    "    image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "    image_features_dict[image_name] = image_features\n",
    "\n",
    "  # Match text queries to images\n",
    "  predictions = {}\n",
    "  with torch.no_grad():\n",
    "    text_features = model.encode_text(text_tokens)\n",
    "    text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "    for i, text_query in enumerate(test_df['Query']):\n",
    "      similarities = {}\n",
    "      for image_name, image_features in image_features_dict.items():\n",
    "        similarity = (100.0 * text_features[i] @ image_features.T).item()\n",
    "        similarities[image_name] = similarity\n",
    "\n",
    "      best_match_image = max(similarities, key=similarities.get)\n",
    "      predictions[text_query] = best_match_image\n",
    "\n",
    "  # Generate unique JSON filename with model_name as prefix\n",
    "  daytime = datetime.datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "  json_file_name = f'BioCLIP_t2i_{daytime}.json'\n",
    "  json_file_path = os.path.join(output_folder_path, json_file_name)\n",
    "\n",
    "  # Save predictions to JSON file\n",
    "  with open(json_file_path, 'w') as json_file:\n",
    "    json.dump(predictions, json_file)\n",
    "\n",
    "  # Create ZIP archive with the same name as the JSON file\n",
    "  zip_file_path = os.path.join(output_folder_path, f'BioCLIP_t2i_{daytime}.zip')\n",
    "  with zipfile.ZipFile(zip_file_path, 'w') as zip_file:\n",
    "    zip_file.write(json_file_path, arcname=json_file_name)\n",
    "\n",
    "  print(f\"Submission file created at: {zip_file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0009ce05",
   "metadata": {},
   "outputs": [],
   "source": [
    "make_submission_cls_task(\n",
    "  model_name=\"hf-hub:magnusdtd/bio-clip-cls-ft\",\n",
    "  model_path=\"\",\n",
    "  test_file_path=\"Dataset/test/cls.csv\",\n",
    "  labels_map = {\n",
    "    \"nose-right\": 0, \n",
    "    \"nose-left\" : 1, \n",
    "    \"ear-right\" : 2, \n",
    "    \"ear-left\"  : 3, \n",
    "    \"vc-open\"   : 4, \n",
    "    \"vc-closed\" : 5, \n",
    "    \"throat\"    : 6, \n",
    "  }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "329c8be9",
   "metadata": {},
   "outputs": [],
   "source": [
    "make_submission_t2i_task(\n",
    "  model_name=\"hf-hub:magnusdtd/bio-clip-cls-ft\",\n",
    "  model_path=\"\",\n",
    "  test_file_path=\"Dataset/test/t2i.csv\",\n",
    "  image_folder_path=\"Dataset/test/imgs\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0396720",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
